# 📌 기능명세서 3: 싱크 분석 및 자막 생성 (OpenAI Whisper API)

## 🟢 프론트엔드 기능명세서

### 1. 화면 레이아웃 및 디자인 명세

- **페이지 위치**: `app/dashboard/page.tsx`
- **컴포넌트 위치**:
  - `components/SubtitleGenerator.tsx` (싱크 분석 버튼 & 자막 출력)
  - `components/ui/button.tsx` (ShadCN `Button` 컴포넌트)
  - `components/ui/progress.tsx` (ShadCN `Progress` 컴포넌트 - 변환 중 표시)
  - `components/ui/textarea.tsx` (ShadCN `Textarea` 컴포넌트 - 자막 표시)

#### 📍 UI 구성 요소

- **"싱크 분석 및 자막 생성" 버튼** (ShadCN `button.tsx` 활용)
  - 클릭 시 Next.js 서버의 API 요청을 보내고, 자막을 변환 후 결과를 받아 화면에 출력
- **변환 진행 상태 표시** (ShadCN `progress.tsx` 활용)
  - API 응답 대기 중 로딩 상태 표시
- **자막 출력 영역** (`textarea.tsx` 활용)
  - Whisper API에서 반환된 자막을 출력

---

### 2. 사용자 흐름 및 상호작용

1. 사용자가 **음성 파일 업로드**
2. **"싱크 분석 및 자막 생성" 버튼 클릭**
3. Next.js 서버의 Route Handler를 통해 OpenAI Whisper API 호출하여 **자막 변환 시작**
4. 변환된 자막을 화면에 표시

---

### 3. API 연동

- 프론트엔드에서 **Next.js Route Handler API**를 호출하여 OpenAI Whisper API 요청
- API 요청 시, `.env` 파일에서 OpenAI API Key를 불러옴

#### 📍 API 요청 형식 (프론트엔드 → Next.js Route Handler)

```ts
const generateSubtitles = async (audioUrl: string) => {
  const response = await fetch("/api/generate-subtitles", {
    method: "POST",
    body: JSON.stringify({ audioUrl }),
    headers: {
      "Content-Type": "application/json",
    },
  });

  const data = await response.json();
  return data.subtitles;
};
```
