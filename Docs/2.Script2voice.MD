# 📌 기능명세서 2: 보이스오버 생성 (ElevenLabs API)

## 🟢 프론트엔드 기능명세서

### 1. 화면 레이아웃 및 디자인 명세

- **페이지 위치**: `app/dashboard/page.tsx`
- **컴포넌트 위치**:
  - `components/VoiceGenerator.tsx` (음성 생성 버튼 & 출력)
  - `components/ui/select.tsx` (ShadCN `Select` 컴포넌트 - 음성 선택)
  - `components/ui/button.tsx` (ShadCN `Button` 컴포넌트)
  - `components/ui/progress.tsx` (ShadCN `Progress` 컴포넌트 - 변환 중 표시)

#### 📍 UI 구성 요소

- **음성 선택 드롭다운** (ShadCN `select.tsx` 활용)
  - 사용자가 선택할 수 있는 AI 음성 목록 표시 (예: `Jenny`, `Matthew`, `Elliot` 등)
- **"음성 생성" 버튼** (ShadCN `button.tsx` 활용)
  - 클릭 시 Next.js 서버의 API 요청을 보내고, 음성을 변환 후 결과를 받아 화면에 출력
- **변환 진행 상태 표시** (ShadCN `progress.tsx` 활용)
  - API 응답 대기 중 로딩 상태 표시
- **오디오 플레이어**
  - 생성된 음성을 재생할 수 있도록 `audio` 태그 활용

---

### 2. 사용자 흐름 및 상호작용

1. 사용자가 **생성된 스크립트 선택**
2. **음성 선택 (예: Jenny, Matthew)**
3. **"음성 생성" 버튼 클릭**
4. Next.js 서버의 Route Handler를 통해 ElevenLabs API 호출하여 **음성 변환 시작**
5. 변환된 음성을 Amazon S3에 업로드 후 다운로드 링크 제공

---

### 3. API 연동

- 프론트엔드에서 **Next.js Route Handler API**를 호출하여 ElevenLabs API 요청
- API 요청 시, `.env` 파일에서 ElevenLabs API Key를 불러옴

#### 📍 API 요청 형식 (프론트엔드 → Next.js Route Handler)

```ts
const generateVoice = async (text: string, voice: string) => {
  const response = await fetch("/api/generate-voice", {
    method: "POST",
    body: JSON.stringify({ text, voice }),
    headers: {
      "Content-Type": "application/json",
    },
  });

  const data = await response.json();
  return data.audioUrl;
};
```
